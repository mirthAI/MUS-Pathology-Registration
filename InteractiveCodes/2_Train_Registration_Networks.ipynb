{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23abbd1d-aca1-4e2b-b0a7-edd540da2343",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "**This notebook walks you through the steps required to train a deep learning architecture for registering micro-US and histopathological images**\n",
    "\n",
    "\n",
    "<h2>\n",
    "    Contents\n",
    "    \n",
    "1. [Imports](#imports)\n",
    "2. [Parameters](#parameters)\n",
    "3. [Training Affine Registration Network](#affinenetwork)\n",
    "4. [Inference for Affine Registration [Optional]](#inference_affine_registration)\n",
    "5. [Training Deformable Registration Network](#deformablenetwork)\n",
    "6. [Inference for Deformable Registration [Optional]](#inference_deformable_registration)   \n",
    "    \n",
    "</h2>\n",
    "    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108437b4-19ca-4126-b09c-e99df5ffbf2e",
   "metadata": {},
   "source": [
    "## 1. Imports <a id=\"imports\"><a>\n",
    "\n",
    "Let't import all the necessary packages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b45c6-f0b0-43ee-9a8b-50f2bafc58e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms  # Import for image transformations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.functional.image import image_gradients\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# User Defined Utilities\n",
    "from utils.Dataset import ImageRegistrationDataset\n",
    "from utils.AffineRegistrationModel import AffineNet\n",
    "from utils.DeformableRegistrationNetwork import DeformRegNet\n",
    "from utils.SpatialTransformation import SpatialTransformer\n",
    "from utils.miscellaneous import apply_affine_transformation, smoothness, loss_function, ssd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2dbd9-7d90-4cbd-ae25-8f5d9658eeea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Uncomment the following cell if you want to print the version of each module!!!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a83b7-1112-49bd-82f8-4d30dbd323b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torchvision, PIL, scipy, matplotlib\n",
    "# from platform import python_version\n",
    "\n",
    "# print(f\"Python Version: {python_version()}\")\n",
    "# print(f\"Open CV Version: {cv2.__version__}\")\n",
    "# print(f\"Numpy Version: {np.__version__}\")\n",
    "# print(f\"PIL Version: {PIL.__version__}\")\n",
    "# print(f\"Matplotlib Version: {matplotlib.__version__}\")\n",
    "# print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "# print(f\"Scipy Version: {scipy.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b669667-8b42-4e1d-8d66-6b6968f839c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If using MIQ_Kernel, we have the follwing versions:\n",
    "#     Python Version: 3.10.14\n",
    "#     Open CV Version: 4.10.0\n",
    "#     Numpy Version: 1.26.4\n",
    "#     PIL Version: 10.3.0\n",
    "#     Matplotlib Version: 3.8.3\n",
    "#     Torchvision Version: 0.15.2\n",
    "#     Scipy Version: 1.12.0\n",
    "#     CSV Version 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b979df-8015-4bbc-8316-a3d816b10913",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[OPTIONAL] Let's visualize the dataset. We will just display some random histopathology and microu-US images and associated masks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56de2cb-8e72-4bfd-861c-d4d54fa45514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_csv_path_file = \"../data/processed_png_data/Training_Label_Paths_For_Fold1.csv\"\n",
    "# dataset = ImageRegistrationDataset(dataset_csv_path_file)\n",
    "# micro_image, micro_mask, hist_image, hist_mask = dataset[3]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(12,5))\n",
    "# plt.subplot(141);plt.imshow(micro_image.permute(1,2,0));plt.title(\"Micro Image\");plt.axis('off');\n",
    "# plt.subplot(142);plt.imshow(micro_mask.permute(1,2,0));plt.title(\"Micro Mask\");plt.axis('off');\n",
    "# plt.subplot(143);plt.imshow(hist_image.permute(1,2,0));plt.title(\"Hist Image\");plt.axis('off');\n",
    "# plt.subplot(144);plt.imshow(hist_mask.permute(1,2,0));plt.title(\"Hist Mask\");plt.axis('off');\n",
    "# plt.tight_layout()\n",
    "# plt.show();\n",
    "# print(f\"Micro Image: Min: {torch.min(micro_image)} | Max: {torch.max(micro_image)} | Shape: {micro_image.shape} | Unique: {len(torch.unique(micro_image))} | DType: {micro_image.dtype}\")\n",
    "# print(f\"Micro Mask: Min: {torch.min(micro_mask)} | Max: {torch.max(micro_mask)} | Shape: {micro_mask.shape} | Unique: {len(torch.unique(micro_mask))} | DType: {micro_mask.dtype}\")\n",
    "# print(f\"Hist Image: Min: {torch.min(hist_image)} | Max: {torch.max(hist_image)} | Shape: {hist_image.shape} | Unique: {len(torch.unique(hist_image))} | DType: {hist_image.dtype}\")\n",
    "# print(f\"Hist Mask: Min: {torch.min(hist_mask)} | Max: {torch.max(hist_mask)} | Shape: {hist_mask.shape} | Unique: {len(torch.unique(hist_mask))} | DType: {hist_mask.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9829bd-2569-40d8-91d1-467f5865e8d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Parameters <a id=\"parameters\"><a>\n",
    "\n",
    "Let's define all the parameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5314b-2a58-4dc2-9f2d-563ab9c9f65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_of_folds = 6\n",
    "saved_model_dir = \"./saved_models/\" #location where the trained models be saved\n",
    "results_dir = \"./results/\" # directory where the result be saved\n",
    "data_dir = \"../data/processed_png_data/\"\n",
    "batch_size = 1\n",
    "os.makedirs(saved_model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684c728-057c-42f2-8625-ea21029217ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "    \n",
    "## 3. Training Affine Registration Network <a id=\"affinenetwork\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "Let's define all the parameters\n",
    "\n",
    "---\n",
    "    \n",
    "### 3.1. Defining the Affine Registration Network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d9511-c46b-4385-a4c1-b313f93598f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define all the parameters\n",
    "affine_model = AffineNet().to(device)\n",
    "trainable_params = sum(p.numel() for p in affine_model.parameters() if p.requires_grad)\n",
    "print(f\"affine model trainable params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaaf8af-4b76-4fe5-a3eb-601ef20b4607",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### 3.2. Training the Affine Registration Network\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d9423-eba4-44e6-81c8-4872db0e1a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_affine(affine_model, train_loader, val_loader, optimizer_affine, criterion_affine, device, path_to_save_the_model, num_epochs=10):\n",
    "    min_loss = float('inf')  # Initialize minimum loss to infinity\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize tqdm for the training loop\n",
    "        train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", dynamic_ncols=True)\n",
    "        \n",
    "        for fixed_image, fixed_mask, moving_image, moving_mask in train_loader_iter:\n",
    "            ############# AFFINE REGISTRATION ###############################\n",
    "            optimizer_affine.zero_grad()\n",
    "            # Pass masks through the network\n",
    "            fixed_mask = fixed_mask.to(device)\n",
    "            moving_mask = moving_mask.to(device)\n",
    "            theta = affine_model((fixed_mask, moving_mask))\n",
    "            # Apply transformation and compute deformed mask\n",
    "            deformed_mask = apply_affine_transformation(moving_mask, theta)\n",
    "            # Calculate SSD loss\n",
    "            loss_affine = criterion_affine(fixed_mask, deformed_mask)\n",
    "            # Backpropagate and update weights\n",
    "            loss_affine.backward()\n",
    "            optimizer_affine.step()\n",
    "            \n",
    "            # Update tqdm description with current loss\n",
    "            train_loader_iter.set_postfix(loss=loss_affine.item())        \n",
    "        \n",
    "        # Save the model if loss decreased\n",
    "        if loss_affine < min_loss:\n",
    "            min_loss = loss_affine.item()\n",
    "            print(\"Saving model with improved loss:\", min_loss)\n",
    "            torch.save(affine_model.state_dict(), path_to_save_the_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e107907-f092-4457-8d38-c0610f2a4a7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "### 3.3. Initializing Training Process for Each Fold\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1e246-c746-4fea-a20c-33b2c4387537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(num_of_folds):\n",
    "    print(f\"Processing Fold {i+1} ...\")\n",
    "    \n",
    "    # Dataset Preparation\n",
    "    train_csv_path_file = os.path.join(data_dir, \"Training_Label_Paths_For_Fold\"+str(i+1)+\".csv\")\n",
    "    test_csv_path_file = os.path.join(data_dir, \"Testing_Label_Paths_For_Fold\"+str(i+1)+\".csv\")\n",
    "    \n",
    "    train_dataset = ImageRegistrationDataset(train_csv_path_file)\n",
    "    test_dataset = ImageRegistrationDataset(test_csv_path_file)\n",
    "    print(f\"Train Dataset: {len(train_dataset)} | Test Dataset: {len(test_dataset)}\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Defining Model\n",
    "    affine_model = AffineNet().to(device)\n",
    "    optimizer_affine = torch.optim.Adam(affine_model.parameters(), lr=0.0001)\n",
    "    criterion_affine = ssd_loss\n",
    "    path_to_save_the_model = os.path.join(saved_model_dir, \"trained_affine_registration_model_for_Fold\"+str(i+1)+\".pth\")\n",
    "    train_affine(affine_model, train_dataloader, val_dataloader, optimizer_affine, criterion_affine, device, path_to_save_the_model, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f5f66-e14c-4f37-92ca-3bb7a6692990",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### 3.4 Evaluation\n",
    "\n",
    "Let's evaluate the training affine model on a single image see how it works!\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39e3c9-d0bb-44ba-98fa-522f34dc8628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_single_image(affine_model, fixed_image, fixed_mask, moving_image, moving_mask, device):\n",
    "    \n",
    "    affine_model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        fixed_mask = fixed_mask.to(device)\n",
    "        moving_mask = moving_mask.to(device)\n",
    "        moving_image = moving_image.to(device)\n",
    "        theta = affine_model((fixed_mask, moving_mask))\n",
    "        deformed_image = apply_affine_transformation(moving_image, theta, mode=\"bilinear\")\n",
    "        \n",
    "    return deformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4743a-ef5d-40e0-a76c-3e6f267dab14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Inference for Affine Registration <a id = \"inference_affine_registration\"></a>\n",
    "\n",
    "[OPTIONAL] If you like, you can check the affine registration network's performance by uncommenting the following cell:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a604250-41c1-4a56-afd1-0b490878e542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Which fold are you interested in?\n",
    "# fold = 1 # 1, 2, 3, 4, 5 or 6\n",
    "# # which image index are you interested in to see the results?\n",
    "# index = 2 # 0, 1, 2, or 3\n",
    "# path_to_affine_model = os.path.join(saved_model_dir, \"trained_affine_registration_model_for_Fold\" + str(fold) + \".pth\")\n",
    "\n",
    "# dataset_csv_path_file = os.path.join(data_dir, \"Testing_Label_Paths_For_Fold\" + str(fold) + \".csv\")\n",
    "# dataset = ImageRegistrationDataset(dataset_csv_path_file)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "# for batch in dataloader:\n",
    "#     batch = batch\n",
    "#     break\n",
    "# fixed_image, fixed_mask, moving_image, moving_mask = batch\n",
    "# fixed_image = fixed_image[index].unsqueeze(0).to(device)\n",
    "# fixed_mask = fixed_mask[index].unsqueeze(0).to(device)\n",
    "# moving_image = moving_image[index].unsqueeze(0).to(device)\n",
    "# moving_mask = moving_mask[index].unsqueeze(0).to(device)\n",
    "# # Loading the best model\n",
    "# trained_affine_model = AffineNet().to(device)\n",
    "# trained_affine_model.load_state_dict(torch.load(path_to_affine_model))\n",
    "# deformed_image = inference_single_image(affine_model,fixed_image, fixed_mask, moving_image, moving_mask, device)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10,6));\n",
    "# plt.subplot(131);plt.imshow(fixed_image[0].permute(1,2,0).detach().cpu());plt.axis('off');plt.title(\"Fixed Image\")\n",
    "# plt.subplot(132);plt.imshow(moving_image[0].permute(1,2,0).detach().cpu());plt.axis('off');plt.title(\"Moving Image\")\n",
    "# plt.subplot(133);plt.imshow(deformed_image[0].permute(1,2,0).detach().cpu());plt.axis('off');plt.title(\"Affine Deformed Image\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb1314-421f-4aae-90e9-84f912f98b5e",
   "metadata": {},
   "source": [
    "---\n",
    "<div align=\"center\">\n",
    "    \n",
    "## 5. Training Deformable Registration Network <a id=\"deformablenetwork\"></a>\n",
    "    \n",
    "</div>\n",
    "    \n",
    "Let's define all the parameters\n",
    "\n",
    "---\n",
    "    \n",
    "### 5.1. Defining the Deformable Registration Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d468c26-7fc4-4052-8f6a-ad3cb019a10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's define all the parameters\n",
    "deformable_model = DeformRegNet(in_channels=6, out_channels=2, init_features=4).to(device)\n",
    "trainable_params = sum(p.numel() for p in deformable_model.parameters() if p.requires_grad)\n",
    "print(f\"Deformable Model Trainable Params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6cae9-5178-41a8-b7d7-c03bb15f21c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.2. Training the Deformable Registration Network\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72581c3-5ec4-4faf-a903-f9917cdcdb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_DeformRegNet(affine_model, deformable_model, train_loader, val_loader, optimizer, criterion, device, path_to_save_the_model, results_path, stn, num_epochs=10):\n",
    "    min_loss = float('inf')  # Initialize minimum loss to infinity    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize tqdm for the training loop\n",
    "        train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", dynamic_ncols=True)\n",
    "        for fixed_image, fixed_mask, moving_image, moving_mask in train_loader_iter:            \n",
    "            optimizer.zero_grad()\n",
    "            fixed_mask = fixed_mask.to(device)\n",
    "            moving_mask = moving_mask.to(device)\n",
    "            fixed_image = fixed_image.to(device)\n",
    "            moving_image = moving_image.to(device)\n",
    "            affine_theta = affine_model((fixed_mask, moving_mask))\n",
    "            affine_deformed_image = apply_affine_transformation(moving_image, affine_theta, mode=\"bilinear\")\n",
    "            affine_deformed_mask = apply_affine_transformation(moving_mask, affine_theta)\n",
    "            input_tensor = torch.cat([affine_deformed_image, fixed_image], dim=1)\n",
    "            flow = deformable_model(input_tensor)\n",
    "            registered_img = stn(affine_deformed_image, flow)\n",
    "            loss_image = criterion(registered_img, fixed_image, flow)\n",
    "            # For mask\n",
    "            registered_mask = stn(affine_deformed_mask, flow)\n",
    "            loss_label = nn.MSELoss()(registered_mask, fixed_mask)\n",
    "            loss = loss_image + loss_label\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            \n",
    "            # Update tqdm description with current loss\n",
    "            train_loader_iter.set_postfix(loss=loss.item()) \n",
    "        \n",
    "        # Save the model if loss decreased\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss.item()\n",
    "            print(\"Saving model with improved loss:\", min_loss)\n",
    "            torch.save(deformable_model.state_dict(), path_to_save_the_model)\n",
    "            \n",
    "            # If you want to save intermediate results for debugging, uncomment the following lines\n",
    "            ######## Saving Results (if you want to save results) ###########\n",
    "            # save_image(moving_image, f'{results_path}/Epoch_{epoch}_Original_Moving_Images.png')\n",
    "            # save_image(moving_mask, f'{results_path}/Epoch_{epoch}_Original_Moving_Masks.png')\n",
    "            # save_image(fixed_image, f'{results_path}/Epoch_{epoch}_Original_Fixed_Images.png')\n",
    "            # save_image(fixed_mask, f'{results_path}/Epoch_{epoch}_Original_Fixed_Masks.png')\n",
    "            # save_image(affine_deformed_image, f'{results_path}/Epoch_{epoch}_Affine_Deformed_Moving_Image.png')\n",
    "            # save_image(registered_img, f'{results_path}/Epoch_{epoch}_Registered_Images.png')    \n",
    "\n",
    "            # torch.save(theta_tps, f'theta_tps_epoch_{epoch}.pt')\n",
    "            # torch.save(affine_theta, f'affine_theta_epoch_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956657d-4884-434f-9a45-d9905e3389a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### 5.3. Initializing Training Process for Each Fold\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40eb36-6397-47ac-9ca4-fc2f005abc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(num_of_folds):\n",
    "    print(f\"Processing Fold {i+1} ...\")\n",
    "    \n",
    "    \n",
    "    # Loading affine model\n",
    "    print(f\"Let's load Affine Trained Model for Fold {i+1}...\")\n",
    "    path_to_affine_model = os.path.join(saved_model_dir, \"trained_affine_registration_model_for_Fold\" + str(i+1) + \".pth\")\n",
    "    trained_affine_model = AffineNet().to(device)\n",
    "    trained_affine_model.load_state_dict(torch.load(path_to_affine_model))\n",
    "    \n",
    "    # Results Directory\n",
    "    results_path = os.path.join(results_dir, \"Fold\" + str(i))\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    \n",
    "    # Dataset Preparation\n",
    "    train_csv_path_file = os.path.join(data_dir, \"Training_Label_Paths_For_Fold\"+str(i+1)+\".csv\")\n",
    "    test_csv_path_file = os.path.join(data_dir, \"Testing_Label_Paths_For_Fold\"+str(i+1)+\".csv\")\n",
    "    \n",
    "    train_dataset = ImageRegistrationDataset(train_csv_path_file)\n",
    "    test_dataset = ImageRegistrationDataset(test_csv_path_file)\n",
    "    print(f\"Train Dataset: {len(train_dataset)} | Test Dataset: {len(test_dataset)}\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Defining Model\n",
    "    deformable_model = DeformRegNet(in_channels=6, out_channels=2, init_features=4).to(device)\n",
    "    stn = SpatialTransformer()\n",
    "    optimizer = torch.optim.Adam(deformable_model.parameters(), lr=0.01)\n",
    "    criterion = loss_function    \n",
    "    path_to_save_deformable_model = os.path.join(saved_model_dir, \"trained_deformable_registration_model_for_Fold\"+str(i+1)+\".pth\")\n",
    "    train_DeformRegNet(affine_model=trained_affine_model, \n",
    "                       deformable_model=deformable_model, \n",
    "                       train_loader=train_dataloader, \n",
    "                       val_loader=val_dataloader, \n",
    "                       optimizer=optimizer, \n",
    "                       criterion=criterion, \n",
    "                       device=device, \n",
    "                       path_to_save_the_model=path_to_save_deformable_model, \n",
    "                       results_path=results_path, \n",
    "                       stn=stn,\n",
    "                       num_epochs=200)\n",
    "    print('')\n",
    "    print('--'*90)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d18888-9d81-4ec6-a984-64e72f86f6aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### 5.4 Evaluation\n",
    "\n",
    "Let's evaluate the training affine model on a single image see how it works!\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8346487-a2aa-4c9d-b043-27649ab22167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deformable_inference_single_image(affine_model, deformable_model, fixed_image, fixed_mask, moving_image, moving_mask, stn, device):\n",
    "    \n",
    "    affine_model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        fixed_mask = fixed_mask.to(device)\n",
    "        moving_mask = moving_mask.to(device)\n",
    "        moving_image = moving_image.to(device)\n",
    "        theta = affine_model((fixed_mask, moving_mask))\n",
    "        affine_deformed_image = apply_affine_transformation(moving_image, theta)\n",
    "        \n",
    "    deformable_model.eval()\n",
    "    with torch.no_grad():        \n",
    "        fixed_image = fixed_image.to(device)\n",
    "        input_tensor = torch.cat([affine_deformed_image, fixed_image], dim=1)\n",
    "        flow = deformable_model(input_tensor)\n",
    "        registered_img = stn(affine_deformed_image, flow)\n",
    "        #######################################################\n",
    "    return registered_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8796f61-c29d-4bf7-bedb-f6b141dc3389",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Inference for Deformable Registration <a id=\"inference_deformable_registration\"></a>\n",
    "\n",
    "Let's define all the parameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23982f5f-48cb-4e96-a658-14359176cda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Which fold are you interested in?\n",
    "fold = 1 # 1, 2, 3, 4, 5 or 6\n",
    "# which image index are you interested in to see the results?\n",
    "index = 0 # 0, 1, 2, or 3\n",
    "\n",
    "# Defining and Loading Trained Affine Model\n",
    "path_to_affine_model = os.path.join(saved_model_dir, \"trained_affine_registration_model_for_Fold\" + str(fold) + \".pth\")\n",
    "trained_affine_model = AffineNet().to(device)\n",
    "trained_affine_model.load_state_dict(torch.load(path_to_affine_model))\n",
    "\n",
    "\n",
    "# Defining and Loading Trained Deformable Registration Network\n",
    "path_to_deformable_model = os.path.join(saved_model_dir, \"trained_deformable_registration_model_for_Fold\"+str(fold)+\".pth\")\n",
    "trained_deformable_model = DeformRegNet(in_channels=6, out_channels=2, init_features=4).to(device)\n",
    "trained_deformable_model.load_state_dict(torch.load(path_to_deformable_model))\n",
    "stn = SpatialTransformer()\n",
    "\n",
    "dataset_csv_path_file = os.path.join(data_dir, \"Testing_Label_Paths_For_Fold\" + str(fold) + \".csv\")\n",
    "dataset = ImageRegistrationDataset(dataset_csv_path_file)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "for batch in dataloader:\n",
    "    batch = batch\n",
    "    break\n",
    "fixed_image, fixed_mask, moving_image, moving_mask = batch\n",
    "fixed_image = fixed_image[index].unsqueeze(0).to(device)\n",
    "fixed_mask = fixed_mask[index].unsqueeze(0).to(device)\n",
    "moving_image = moving_image[index].unsqueeze(0).to(device)\n",
    "moving_mask = moving_mask[index].unsqueeze(0).to(device)\n",
    "# Loading the best model\n",
    "trained_affine_model = AffineNet().to(device)\n",
    "trained_affine_model.load_state_dict(torch.load(path_to_affine_model))\n",
    "registered_image = deformable_inference_single_image(affine_model=trained_affine_model, \n",
    "                                                   deformable_model=trained_deformable_model,\n",
    "                                                   fixed_image=fixed_image, \n",
    "                                                   fixed_mask=fixed_mask, \n",
    "                                                   moving_image=moving_image, \n",
    "                                                   moving_mask=moving_mask, \n",
    "                                                   stn=stn, \n",
    "                                                   device=device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6));\n",
    "plt.subplot(131);plt.imshow(fixed_image[0].permute(1,2,0).detach().cpu());plt.axis('off');plt.title(\"Fixed Image\")\n",
    "plt.subplot(132);plt.imshow(moving_image[0].permute(1,2,0).detach().cpu());plt.axis('off');plt.title(\"Moving Image\")\n",
    "plt.subplot(133);plt.imshow(registered_image[0].permute(1,2,0).detach().cpu());plt.axis('off');plt.title(\"Registered Image\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def threshold_binary(image, threshold=0.001):\n",
    "    \"\"\"\n",
    "    Threshold the input image to convert it into a binary mask.\n",
    "    \"\"\"\n",
    "    return (image > threshold).float()\n",
    "\n",
    "def dice_coefficient(image1, image2):\n",
    "    \"\"\"\n",
    "    Compute the Dice coefficient between two binary masks.\n",
    "    \"\"\"\n",
    "    intersection = torch.sum(image1 * image2)\n",
    "    union = torch.sum(image1) + torch.sum(image2)\n",
    "    dice = (2. * intersection) / (union + 1e-8)  # Add a small epsilon to avoid division by zero\n",
    "    return dice\n",
    "\n",
    "# Assuming registered_image, fixed_image, and moving_image are torch tensors\n",
    "# and they are already thresholded if necessary\n",
    "image_binary = threshold_binary(fixed_image)\n",
    "registered_image_binary = threshold_binary(moving_image)\n",
    "recovered_image_binary = threshold_binary(registered_image)\n",
    "\n",
    "\n",
    "dice1 = dice_coefficient(image_binary.detach().cpu(), registered_image_binary.detach().cpu())\n",
    "dice2 = dice_coefficient(image_binary.detach().cpu(), recovered_image_binary.detach().cpu())\n",
    "\n",
    "print(\"Dice coefficient between registered_image and fixed_image:\", dice1.item())\n",
    "print(\"Dice coefficient between fixed_image and moving_image:\", dice2.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIQ_Kernel",
   "language": "python",
   "name": "miq_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
